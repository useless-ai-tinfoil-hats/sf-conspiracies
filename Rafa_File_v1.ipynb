{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OvkPji9O-qX"
      },
      "source": [
        "# Tutorial: Creating Your First QA Pipeline with Retrieval-Augmentation\n",
        "\n",
        "- **Level**: Beginner\n",
        "- **Time to complete**: 10 minutes\n",
        "- **Components Used**: [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore), [`SentenceTransformersDocumentEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder), [`SentenceTransformersTextEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder), [`InMemoryEmbeddingRetriever`](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever), [`PromptBuilder`](https://docs.haystack.deepset.ai/docs/promptbuilder), [`OpenAIGenerator`](https://docs.haystack.deepset.ai/docs/openaigenerator)\n",
        "- **Prerequisites**: You must have an [OpenAI API Key](https://platform.openai.com/api-keys).\n",
        "- **Goal**: After completing this tutorial, you'll have learned the new prompt syntax and how to use PromptBuilder and OpenAIGenerator to build a generative question-answering pipeline with retrieval-augmentation.\n",
        "\n",
        "> This tutorial uses Haystack 2.0. To learn more, read the [Haystack 2.0 announcement](https://haystack.deepset.ai/blog/haystack-2-release) or visit the [Haystack 2.0 Documentation](https://docs.haystack.deepset.ai/docs/intro)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFqHcXYPO-qZ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows you how to create a generative question-answering pipeline using the retrieval-augmentation ([RAG](https://www.deepset.ai/blog/llms-retrieval-augmentation)) approach with Haystack 2.0. The process involves four main components: [SentenceTransformersTextEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder) for creating an embedding for the user query, [InMemoryBM25Retriever](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever) for fetching relevant documents, [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder) for creating a template prompt, and [OpenAIGenerator](https://docs.haystack.deepset.ai/docs/openaigenerator) for generating responses.\n",
        "\n",
        "For this tutorial, you'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents, but you can replace them with any text you want.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXjVlbPiO-qZ"
      },
      "source": [
        "## Preparing the Colab Environment\n",
        "\n",
        "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration)\n",
        "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/logging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kww5B_vXO-qZ"
      },
      "source": [
        "## Installing Haystack\n",
        "\n",
        "Install Haystack 2.0 and other required packages with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQbU8GUfO-qZ",
        "outputId": "c33579e9-5557-43bd-a3c5-63b8373770c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting haystack-experimental (from haystack-ai)\n",
            "  Downloading haystack_experimental-0.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (3.1.4)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (3.2.1)\n",
            "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (1.35.9)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (2.2.2)\n",
            "Requirement already satisfied: posthog in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (6.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (8.4.2)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from haystack-ai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->haystack-ai) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->haystack-ai) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from posthog->haystack-ai) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from posthog->haystack-ai) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->haystack-ai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->haystack-ai) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->haystack-ai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.20.0)\n",
            "Downloading haystack_ai-2.4.0-py3-none-any.whl (350 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.7/350.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading haystack_experimental-0.1.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: lazy-imports, haystack-experimental, haystack-ai\n",
            "Successfully installed haystack-ai-2.4.0 haystack-experimental-0.1.1 lazy-imports-0.3.1\n",
            "Collecting datasets>=2.6.1\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.6.1)\n",
            "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.6.1)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.6.1) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (0.23.4)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.6.1) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.6.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.6.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.6.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.6.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.6.1) (1.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets>=2.6.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.6.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.6.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.6.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.6.1) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.6.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.6.1) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.6.1) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.6.1) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
            "Installing collected packages: xxhash, multiprocess, datasets\n",
            "Successfully installed datasets-2.21.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting sentence-transformers>=3.0.0\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers>=3.0.0)\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (2.3.1)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (1.4.2)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=3.0.0) (10.4.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=3.0.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=3.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=3.0.0) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=3.0.0) (2023.12.25)\n",
            "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers>=3.0.0)\n",
            "  Downloading safetensors-0.4.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=3.0.0) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=3.0.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=3.0.0) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=3.0.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=3.0.0) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=3.0.0) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.4-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.8/381.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: safetensors, transformers, sentence-transformers\n",
            "Successfully installed safetensors-0.4.4 sentence-transformers-3.0.1 transformers-4.44.2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai\n",
        "pip install \"datasets>=2.6.1\"\n",
        "pip install \"sentence-transformers>=3.0.0\"\n",
        "pip install assemblyai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lvfew16O-qa"
      },
      "source": [
        "## Fetching and Indexing Documents\n",
        "\n",
        "You'll start creating your question answering system by downloading the data and indexing the data with its embeddings to a DocumentStore. \n",
        "\n",
        "In this tutorial, you will take a simple approach to writing documents and their embeddings into the DocumentStore. For a full indexing pipeline with preprocessing, cleaning and splitting, check out our tutorial on [Preprocessing Different File Types](https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline).\n",
        "\n",
        "\n",
        "### Initializing the DocumentStore\n",
        "\n",
        "Initialize a DocumentStore to index your documents. A DocumentStore stores the Documents that the question answering system uses to find answers to your questions. In this tutorial, you'll be using the `InMemoryDocumentStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CbVN-s5LO-qa"
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8nuJdWO-qa"
      },
      "source": [
        "> `InMemoryDocumentStore` is the simplest DocumentStore to get started with. It requires no external dependencies and it's a good option for smaller projects and debugging. But it doesn't scale up so well to larger Document collections, so it's not a good choice for production systems. To learn more about the different types of external databases that Haystack supports, see [DocumentStore Integrations](https://haystack.deepset.ai/integrations?type=Document+Store)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLVaFHTO-qb"
      },
      "source": [
        "The DocumentStore is now ready. Now it's time to fill it with some Documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HryYZP9ZO-qb"
      },
      "source": [
        "### Fetch the Data\n",
        "\n",
        "You'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents. We preprocessed the data and uploaded to a Hugging Face Space: [Seven Wonders](https://huggingface.co/datasets/bilgeyucel/seven-wonders). Thus, you don't need to perform any additional cleaning or splitting.\n",
        "\n",
        "Fetch the data and convert it into Haystack Documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 22785 documents.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from haystack import Document\n",
        "\n",
        "# Path to the directory containing the .txt files\n",
        "folder_path = 'scraped_articles'\n",
        "\n",
        "# List to store the Document objects\n",
        "docs = []\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        \n",
        "        # Read the contents of the file\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "            # Assuming the file structure:\n",
        "            # 1st line: Title\n",
        "            # 2nd line: Separator (-----)\n",
        "            # 3rd line: Summary\n",
        "            title = lines[0].strip()  # Title is on the first line\n",
        "            summary = lines[2].strip()  # Summary is on the third line\n",
        "            \n",
        "            # Create a Document object\n",
        "            doc = Document(content=summary, meta={\"title\": title})\n",
        "            docs.append(doc)\n",
        "\n",
        "# Now `docs` contains all the Document objects that you can use with haystack\n",
        "print(f\"Loaded {len(docs)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INdC3WvLO-qb",
        "outputId": "1af43d0f-2999-4de4-d152-b3cca9fb49e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' from datasets import load_dataset\\nfrom haystack import Document\\n\\ndataset = load_dataset(\"scrapped_articles\", split=\"train\")\\ndocs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset] '"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "dataset = load_dataset(\"scrapped_articles\", split=\"train\")\n",
        "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset] \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' import os\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Number of pages you want to scrape\\npages = 1950\\n\\n# Directory to save the output files\\noutput_dir = \"scraped_articles\"\\n\\n# Create the directory if it doesn\\'t exist\\nif not os.path.exists(output_dir):\\n    os.makedirs(output_dir)\\n\\n# Loop through the number of pages\\nfor i in range(pages):\\n    datasetURL = f\"https://sfchronicle.newsbank.com/search?text=&content_added=2023-09-02&date_from=&date_to=&pub%5B0%5D=SFCWS&sort=new&page={i+1}\"\\n    \\n    if i == 0:  # For the first page, use a different URL pattern\\n        datasetURL = \"https://sfchronicle.newsbank.com/search?text=&content_added=2023-09-02&date_from=&date_to=&pub%5B%5D=SFCWS&sort=new\"\\n    \\n    # Request the page content\\n    response = requests.get(datasetURL)\\n    soup = BeautifulSoup(response.text, \"html.parser\")\\n    \\n    # Find all article containers\\n    articles = soup.find_all(\"div\", class_=\"views-row\")\\n\\n    # Loop through each article container to extract title and summary\\n    for article in articles:\\n        # Extract the title\\n        title_tag = article.find(\"div\", class_=\"views-field views-field-text-1\")\\n        title = title_tag.find(\"a\", class_=\"text-links\").get_text(strip=True) if title_tag else \"No title found\"\\n        \\n        # Extract the summary\\n        summary_tag = article.find(\"div\", class_=\"views-field views-field-text-6\")\\n        summary = summary_tag.find(\"span\", class_=\"field-content\").get_text(strip=True) if summary_tag else \"No summary found\"\\n        \\n        # Sanitize title to use as a filename\\n        safe_title = \"\".join([c for c in title if c.isalpha() or c.isdigit() or c in [\\' \\', \\'.\\', \\'_\\']]).rstrip()\\n        file_path = os.path.join(output_dir, f\"{safe_title}.txt\")\\n        \\n        # Write the content to a text file\\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\\n            f.write(title + \"\\n\")\\n            f.write(\"-----\\n\")\\n            f.write(summary + \"\\n\")\\n\\n        print(f\"Saved: {file_path}\") '"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Webscraper of the SF Chronicle (Last years articles)\n",
        "\"\"\" import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Number of pages you want to scrape\n",
        "pages = 1950\n",
        "\n",
        "# Directory to save the output files\n",
        "output_dir = \"scraped_articles\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Loop through the number of pages\n",
        "for i in range(pages):\n",
        "    datasetURL = f\"https://sfchronicle.newsbank.com/search?text=&content_added=2023-09-02&date_from=&date_to=&pub%5B0%5D=SFCWS&sort=new&page={i+1}\"\n",
        "    \n",
        "    if i == 0:  # For the first page, use a different URL pattern\n",
        "        datasetURL = \"https://sfchronicle.newsbank.com/search?text=&content_added=2023-09-02&date_from=&date_to=&pub%5B%5D=SFCWS&sort=new\"\n",
        "    \n",
        "    # Request the page content\n",
        "    response = requests.get(datasetURL)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    \n",
        "    # Find all article containers\n",
        "    articles = soup.find_all(\"div\", class_=\"views-row\")\n",
        "\n",
        "    # Loop through each article container to extract title and summary\n",
        "    for article in articles:\n",
        "        # Extract the title\n",
        "        title_tag = article.find(\"div\", class_=\"views-field views-field-text-1\")\n",
        "        title = title_tag.find(\"a\", class_=\"text-links\").get_text(strip=True) if title_tag else \"No title found\"\n",
        "        \n",
        "        # Extract the summary\n",
        "        summary_tag = article.find(\"div\", class_=\"views-field views-field-text-6\")\n",
        "        summary = summary_tag.find(\"span\", class_=\"field-content\").get_text(strip=True) if summary_tag else \"No summary found\"\n",
        "        \n",
        "        # Sanitize title to use as a filename\n",
        "        safe_title = \"\".join([c for c in title if c.isalpha() or c.isdigit() or c in [' ', '.', '_']]).rstrip()\n",
        "        file_path = os.path.join(output_dir, f\"{safe_title}.txt\")\n",
        "        \n",
        "        # Write the content to a text file\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(title + \"\\n\")\n",
        "            f.write(\"-----\\n\")\n",
        "            f.write(summary + \"\\n\")\n",
        "\n",
        "        print(f\"Saved: {file_path}\") \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czMjWwnxPA-3"
      },
      "source": [
        "### Initalize a Document Embedder\n",
        "\n",
        "To store your data in the DocumentStore with embeddings, initialize a [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder) with the model name and call `warm_up()` to download the embedding model.\n",
        "\n",
        "> If you'd like, you can use a different [Embedder](https://docs.haystack.deepset.ai/docs/embedders) for your documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUmAH9sEn3R7",
        "outputId": "ee54b59b-4d4a-45eb-c1a9-0b7b248f1dd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y4iJE_SrS4K"
      },
      "source": [
        "### Write Documents to the DocumentStore\n",
        "\n",
        "Run the `doc_embedder` with the Documents. The embedder will create embeddings for each document and save these embeddings in Document object's `embedding` field. Then, you can write the Documents to the DocumentStore with `write_documents()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7d482188c12d4a7886f20a65d3402c59",
            "2a3ec74419ae4a02ac0210db66133415",
            "ddeff9a822404adbbc3cad97a939bc0c",
            "36d341ab3a044709b5af2e8ab97559bc",
            "88fc33e1ab78405e911b5eafa512c935",
            "91e5d4b0ede848319ef0d3b558d57d19",
            "d2428c21707d43f2b6f07bfafbace8bb",
            "7fdb2c859e454e72888709a835f7591e",
            "6b8334e071a3438397ba6435aac69f58",
            "5f5cfa425cac4d37b2ea29e53b4ed900",
            "3c59a82dac5c476b9a3e3132094e1702"
          ]
        },
        "id": "ETpQKftLplqh",
        "outputId": "b9c8658c-90c8-497c-e765-97487c0daf8e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d82db5a5dfc444f9c4629d49f0c68fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/713 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "22785"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdojTxg6uubn"
      },
      "source": [
        "## Building the RAG Pipeline\n",
        "\n",
        "The next step is to build a [Pipeline](https://docs.haystack.deepset.ai/docs/pipelines) to generate answers for the user query following the RAG approach. To create the pipeline, you first need to initialize each component, add them to your pipeline, and connect them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uyV6-u-u56P"
      },
      "source": [
        "### Initialize a Text Embedder\n",
        "\n",
        "Initialize a text embedder to create an embedding for the user query. The created embedding will later be used by the Retriever to retrieve relevant documents from the DocumentStore.\n",
        "\n",
        "> ⚠️ Notice that you used `sentence-transformers/all-MiniLM-L6-v2` model to create embeddings for your documents before. This is why you need to use the same model to embed the user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LyJY2yW628dl"
      },
      "outputs": [],
      "source": [
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_cj-5m-O-qb"
      },
      "source": [
        "### Initialize the Retriever\n",
        "\n",
        "Initialize a [InMemoryEmbeddingRetriever](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever) and make it use the InMemoryDocumentStore you initialized earlier in this tutorial. This Retriever will get the relevant documents to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-uo-6fjiO-qb"
      },
      "outputs": [],
      "source": [
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CEuQpB7O-qb"
      },
      "source": [
        "### Define a Template Prompt\n",
        "\n",
        "Create a custom prompt for a generative question answering task using the RAG approach. The prompt should take in two parameters: `documents`, which are retrieved from a document store, and a `question` from the user. Use the Jinja2 looping syntax to combine the content of the retrieved documents in the prompt.\n",
        "\n",
        "Next, initialize a [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder) instance with your prompt template. The PromptBuilder, when given the necessary values, will automatically fill in the variable values and generate a complete prompt. This approach allows for a more tailored and effective question-answering experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ObahTh45FqOT"
      },
      "outputs": [],
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "template = \"\"\"\n",
        "Consider the context given in the question, please generate a wild conspiracy theory in two paragraphs that could only occur based on the following news articles of events that occurred in San Francisco in the last year \n",
        "\n",
        "\n",
        "Consider:\n",
        "1. At the end of the answer, give me the title of the documents you used and how the story relates to the conspiracy theory.\n",
        "2. The documents are divided into Title, a separator (-----) and Summary.\n",
        "\n",
        "The context for the question are the following news articles of events that occurred in San Francisco in the last year, consider them as highly important to properly answer the question:\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{question}}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR14lbfcFtXj"
      },
      "source": [
        "### Initialize a Generator\n",
        "\n",
        "\n",
        "Generators are the components that interact with large language models (LLMs). Now, set `OPENAI_API_KEY` environment variable and initialize a [OpenAIGenerator](https://docs.haystack.deepset.ai/docs/OpenAIGenerator) that can communicate with OpenAI GPT models. As you initialize, provide a model name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SavE_FAqfApo",
        "outputId": "1afbf2e8-ae63-41ff-c37f-5123b2103356"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
        "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nenbo2SvycHd"
      },
      "source": [
        "> You can replace `OpenAIGenerator` in your pipeline with another `Generator`. Check out the full list of generators [here](https://docs.haystack.deepset.ai/docs/generators)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bfHwOQwycHe"
      },
      "source": [
        "### Build the Pipeline\n",
        "\n",
        "To build a pipeline, add all components to your pipeline and connect them. Create connections from `text_embedder`'s \"embedding\" output to \"query_embedding\" input of `retriever`, from `retriever` to `prompt_builder` and from `prompt_builder` to `llm`. Explicitly connect the output of `retriever` with \"documents\" input of the `prompt_builder` to make the connection obvious as `prompt_builder` has two inputs (\"documents\" and \"question\").\n",
        "\n",
        "For more information on pipelines and creating connections, refer to [Creating Pipelines](https://docs.haystack.deepset.ai/docs/creating-pipelines) documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f6NFmpjEO-qb",
        "outputId": "89fd1b48-5189-4401-9cf8-15f55c503676"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x3f107a300>\n",
              "🚅 Components\n",
              "  - text_embedder: SentenceTransformersTextEmbedder\n",
              "  - retriever: InMemoryEmbeddingRetriever\n",
              "  - prompt_builder: PromptBuilder\n",
              "  - llm: OpenAIGenerator\n",
              "🛤️ Connections\n",
              "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
              "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
              "  - prompt_builder.prompt -> llm.prompt (str)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "basic_rag_pipeline = Pipeline()\n",
        "# Add components to your pipeline\n",
        "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
        "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "basic_rag_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "# Now, connect the components to each other\n",
        "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NqyLhx7O-qc"
      },
      "source": [
        "That's it! Your RAG pipeline is ready to generate answers to questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBAyF5tVO-qc"
      },
      "source": [
        "## Asking a Question\n",
        "\n",
        "When asking a question, use the `run()` method of the pipeline. Make sure to provide the question to both the `text_embedder` and the `prompt_builder`. This ensures that the `{{question}}` variable in the template prompt gets replaced with your specific question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "4e6e97b6d54f4f80bb7e8b25aba8e616",
            "1a820c06a7a049d8b6c9ff300284d06e",
            "58ff4e0603a74978a134f63533859be5",
            "8bdb8bfae31d4f4cb6c3b0bf43120eed",
            "39a68d9a5c274e2dafaa2d1f86eea768",
            "d0cfe5dacdfc431a91b4c4741123e2d0",
            "e7f1e1a14bb740d18827dd78bbe7b2e3",
            "3fda06f905b445a488efdd2dd08c0939",
            "2bc341a780f7498ba9cd475468841bb5",
            "d7218475e23b420a8c03d00ca4ab8718",
            "a694abaf765f4d1b82fa0138e59c6793"
          ]
        },
        "id": "Vnt283M5O-qc",
        "outputId": "d2843a73-3ad5-4daa-8d1e-a58de7aa2bb0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e425ca4077a46d38384bdd34cf4041c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a wild conspiracy theory, it appears that the San Francisco 49ers are actually working in cahoots with other NFL teams to manipulate game outcomes in order to benefit financially from bets placed on their games. The team's inconsistent performance and unexpected losses are not a result of natural gameplay, but rather carefully orchestrated schemes to deceive the public and cover up their illicit activities. The 49ers' recent defeat to the Baltimore Ravens, which was seen as a turning point for the team, was actually a strategic move to throw off suspicion and maintain their facade of competition.\n",
            "\n",
            "This theory is supported by the articles \"Somewhere between the extremes,\" \"The San Francisco 49ers appear primed for a Super Bowl run,\" and \"John Lynch's response to an ESPN report,\" which highlight the unpredictability and questionable behavior surrounding the team. The mention of 'juicy scenarios' in the context of contract negotiations and draft picks suggests that there may be hidden agendas at play within the organization. Additionally, the 49ers' surprising victories and losses could be seen as intentional efforts to manipulate betting odds and profit off the outcomes. It's clear that there is more to the San Francisco 49ers' performance than meets the eye, and their true intentions may be far more sinister than anyone realizes.\n"
          ]
        }
      ],
      "source": [
        "question =  \"Could you generate a conspiracy theory based on the current state of the San Francisco 49ers football team?\"\n",
        "\n",
        "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQN-aoGO-qc"
      },
      "source": [
        "Here are some other example questions to test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_OHUQ5xxO-qc"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    \"Generate a conspiracy theory based on the most trendy restaurants in San Francisco\",\n",
        "    'Generate a consipracy theory based on San Francisco\\'s Chinatown',\n",
        "    'Generate a consipracy theory based on the state of San Francisco\\'s police',\n",
        "    'Generate a consipracy theory about the current state of music in San Facisco',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XueCK3y4O-qc"
      },
      "source": [
        "## What's next\n",
        "\n",
        "🎉 Congratulations! You've learned how to create a generative QA system for your documents with the RAG approach.\n",
        "\n",
        "If you liked this tutorial, you may also enjoy:\n",
        "- [Filtering Documents with Metadata](https://haystack.deepset.ai/tutorials/31_metadata_filtering)\n",
        "- [Preprocessing Different File Types](https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline)\n",
        "- [Creating a Hybrid Retrieval Pipeline](https://haystack.deepset.ai/tutorials/33_hybrid_retrieval)\n",
        "\n",
        "To stay up to date on the latest Haystack developments, you can [subscribe to our newsletter](https://landing.deepset.ai/haystack-community-updates) and [join Haystack discord community](https://discord.gg/haystack).\n",
        "\n",
        "Thanks for reading!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Integrate with Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97cfa9ca34a44698b3ec7c6fbf5375fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a shocking twist, it has been uncovered that the decline of the music scene in San Francisco is not due to natural causes, but rather a carefully orchestrated plan by a secret society known as the \"Doom Loop Syndicate\". This group of elites, made up of influential figures in the tech industry and political sphere, has been systematically erasing the city's musical identity in favor of a more homogenized, mainstream sound. Through their control of major music festivals like Outside Lands and Noise Pop, they have been manipulating the music culture to fit their own agenda, stifling creativity and diversity in the process.\n",
            "\n",
            "The key to their plan lies in the manipulation of iconic figures in San Francisco music history, such as Sly Stone, whose downfall was orchestrated to send a message to other artists who dared to challenge the status quo. By promoting a narrative of artistic decline and cultural extinction, the Doom Loop Syndicate aims to control the narrative and steer the city's cultural identity in a direction that aligns with their own interests. Through the use of propaganda and manipulation, they have successfully convinced the public that San Francisco's music scene is on the brink of extinction, when in reality, it is a carefully manufactured illusion designed to maintain their grip on power and influence.\n",
            "\n",
            "Documents used:\n",
            "- \"San Francisco's culture is alive and well, and we're here to prove it\"\n",
            "- \"San Francisco music history is full of notorious characters, but none shine as brightly and faded as dramatically as Sly Stone\"\n"
          ]
        }
      ],
      "source": [
        "question =  \"Generate a consipracy theory about the current state of music in San Francisco\"\n",
        "\n",
        "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a820c06a7a049d8b6c9ff300284d06e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0cfe5dacdfc431a91b4c4741123e2d0",
            "placeholder": "​",
            "style": "IPY_MODEL_e7f1e1a14bb740d18827dd78bbe7b2e3",
            "value": "Batches: 100%"
          }
        },
        "2a3ec74419ae4a02ac0210db66133415": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e5d4b0ede848319ef0d3b558d57d19",
            "placeholder": "​",
            "style": "IPY_MODEL_d2428c21707d43f2b6f07bfafbace8bb",
            "value": "Batches: 100%"
          }
        },
        "2bc341a780f7498ba9cd475468841bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36d341ab3a044709b5af2e8ab97559bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f5cfa425cac4d37b2ea29e53b4ed900",
            "placeholder": "​",
            "style": "IPY_MODEL_3c59a82dac5c476b9a3e3132094e1702",
            "value": " 5/5 [00:01&lt;00:00,  3.35it/s]"
          }
        },
        "39a68d9a5c274e2dafaa2d1f86eea768": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c59a82dac5c476b9a3e3132094e1702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fda06f905b445a488efdd2dd08c0939": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6e97b6d54f4f80bb7e8b25aba8e616": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a820c06a7a049d8b6c9ff300284d06e",
              "IPY_MODEL_58ff4e0603a74978a134f63533859be5",
              "IPY_MODEL_8bdb8bfae31d4f4cb6c3b0bf43120eed"
            ],
            "layout": "IPY_MODEL_39a68d9a5c274e2dafaa2d1f86eea768"
          }
        },
        "58ff4e0603a74978a134f63533859be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fda06f905b445a488efdd2dd08c0939",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bc341a780f7498ba9cd475468841bb5",
            "value": 1
          }
        },
        "5f5cfa425cac4d37b2ea29e53b4ed900": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b8334e071a3438397ba6435aac69f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d482188c12d4a7886f20a65d3402c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a3ec74419ae4a02ac0210db66133415",
              "IPY_MODEL_ddeff9a822404adbbc3cad97a939bc0c",
              "IPY_MODEL_36d341ab3a044709b5af2e8ab97559bc"
            ],
            "layout": "IPY_MODEL_88fc33e1ab78405e911b5eafa512c935"
          }
        },
        "7fdb2c859e454e72888709a835f7591e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88fc33e1ab78405e911b5eafa512c935": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bdb8bfae31d4f4cb6c3b0bf43120eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7218475e23b420a8c03d00ca4ab8718",
            "placeholder": "​",
            "style": "IPY_MODEL_a694abaf765f4d1b82fa0138e59c6793",
            "value": " 1/1 [00:00&lt;00:00, 18.42it/s]"
          }
        },
        "91e5d4b0ede848319ef0d3b558d57d19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a694abaf765f4d1b82fa0138e59c6793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0cfe5dacdfc431a91b4c4741123e2d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2428c21707d43f2b6f07bfafbace8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7218475e23b420a8c03d00ca4ab8718": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddeff9a822404adbbc3cad97a939bc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fdb2c859e454e72888709a835f7591e",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b8334e071a3438397ba6435aac69f58",
            "value": 5
          }
        },
        "e7f1e1a14bb740d18827dd78bbe7b2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
